# Performance Benchmarks & Regression Detection
# Re-enabled for enterprise production readiness
name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance benchmarks weekly on Sundays at 2 AM UTC (reduced from daily)
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      update_baselines:
        description: 'Update performance baselines'
        required: false
        default: false
        type: boolean
      benchmark_filter:
        description: 'Benchmark filter pattern'
        required: false
        default: ''
        type: string

env:
  CARGO_TERM_COLOR: always
  # Stable benchmark environment
  CARGO_TARGET_DIR: ${{ github.workspace }}/target
  RUST_BACKTRACE: 1

jobs:
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Full history for baseline comparison
        fetch-depth: 0

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable

    - name: Cache cargo registry
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-registry-

    - name: Cache cargo build
      uses: actions/cache@v4
      with:
        path: target
        key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-bench-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential pkg-config protobuf-compiler

    - name: Setup benchmark environment
      run: |
        # Set environment variables for consistent benchmarking
        echo "RUSTC_VERSION=$(rustc --version)" >> $GITHUB_ENV
        echo "GIT_COMMIT=${{ github.sha }}" >> $GITHUB_ENV
        echo "CPU_MODEL=$(cat /proc/cpuinfo | grep 'model name' | head -1 | cut -d: -f2)" >> $GITHUB_ENV
        echo "PERFORMANCE_THRESHOLD=5" >> $GITHUB_ENV

        # Create results directory
        mkdir -p benches/results/baselines

    - name: Restore performance baselines
      uses: actions/cache@v4
      with:
        path: benches/results/baselines
        key: performance-baselines-${{ runner.os }}-v1
        restore-keys: |
          performance-baselines-${{ runner.os }}-

    - name: Build project in release mode
      run: cargo build --release --all-features

    - name: Run performance benchmarks
      run: |
        if [[ "${{ github.event_name }}" == "schedule" || "${{ github.event.inputs.update_baselines }}" == "true" ]]; then
          echo "Running benchmarks with baseline update"
          ./scripts/run_benchmarks.sh baseline --format json
        elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
          echo "Running regression detection for PR"
          ./scripts/run_benchmarks.sh ci --format json
        else
          echo "Running full benchmark suite"
          ./scripts/run_benchmarks.sh run --format json
        fi
      env:
        BENCHMARK_FILTER: ${{ github.event.inputs.benchmark_filter }}

    - name: Save performance baselines
      if: github.event_name == 'schedule' || github.event.inputs.update_baselines == 'true'
      uses: actions/cache/save@v4
      with:
        path: benches/results/baselines
        key: performance-baselines-${{ runner.os }}-v1-${{ github.sha }}

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benches/results/
          target/criterion/
        retention-days: 30

    - name: Performance regression check
      if: github.event_name == 'pull_request'
      run: |
        # Run regression detection with strict thresholds for PRs
        export PERFORMANCE_THRESHOLD=3  # 3% threshold for PRs
        ./scripts/run_benchmarks.sh regression

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = 'benches/results/performance_summary.json';

          if (fs.existsSync(path)) {
            const results = JSON.parse(fs.readFileSync(path, 'utf8'));

            const comment = `## üìä Performance Benchmark Results

          **Commit:** \`${{ github.sha }}\`
          **Environment:** Ubuntu Latest (GitHub Actions)

          ### üéØ Key Metrics
          - **Message Creation:** ${results.message_creation || 'N/A'}
          - **JSON Parsing:** ${results.json_parsing || 'N/A'}
          - **Schema Validation:** ${results.schema_validation || 'N/A'}
          - **Context Creation:** ${results.context_creation || 'N/A'}

          ### ‚úÖ Regression Status
          ${results.regressions_detected ? '‚ùå Performance regressions detected!' : '‚úÖ No performance regressions detected'}

          <details>
          <summary>üìà Detailed Results</summary>

          \`\`\`json
          ${JSON.stringify(results, null, 2)}
          \`\`\`
          </details>

          ---
          *Benchmarks run automatically on every PR. [View full results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  memory-benchmarks:
    name: Memory Usage Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable

    - name: Install valgrind and protoc
      run: sudo apt-get update && sudo apt-get install -y valgrind protobuf-compiler

    - name: Cache cargo build
      uses: actions/cache@v4
      with:
        path: target
        key: ${{ runner.os }}-cargo-memory-${{ hashFiles('**/Cargo.lock') }}

    - name: Build for memory analysis
      run: cargo build --release --all-features

    - name: Run memory benchmarks
      run: |
        # Run memory-specific benchmarks
        cargo bench --bench zero_copy_bench -- --output-format json > memory_results.json

        # Analyze memory usage patterns
        valgrind --tool=massif --stacks=yes cargo bench --bench zero_copy_bench -- --test

    - name: Upload memory analysis
      uses: actions/upload-artifact@v4
      with:
        name: memory-analysis-${{ github.sha }}
        path: |
          memory_results.json
          massif.out.*
        retention-days: 7

  platform-benchmarks:
    name: Cross-Platform Performance
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]

    runs-on: ${{ matrix.os }}
    timeout-minutes: 25

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable

    - name: Cache cargo build
      uses: actions/cache@v4
      with:
        path: target
        key: ${{ runner.os }}-cargo-platform-${{ hashFiles('**/Cargo.lock') }}

    - name: Install protoc (Ubuntu)
      if: runner.os == 'Linux'
      run: sudo apt-get update && sudo apt-get install -y protobuf-compiler

    - name: Install protoc (macOS)
      if: runner.os == 'macOS'
      run: brew install protobuf

    - name: Install protoc (Windows)
      if: runner.os == 'Windows'
      uses: arduino/setup-protoc@v3

    - name: Setup platform-specific environment
      shell: bash
      run: |
        if [[ "${{ runner.os }}" == "Windows" ]]; then
          echo "RUSTFLAGS=-C target-feature=+crt-static" >> $GITHUB_ENV
        fi

    - name: Build project
      run: cargo build --release --all-features

    - name: Run core benchmarks
      run: |
        # Run a subset of benchmarks for platform comparison
        cargo bench --package turbomcp-core --bench zero_copy_bench
        cargo bench --package turbomcp --bench performance_tests

    - name: Upload platform results
      uses: actions/upload-artifact@v4
      with:
        name: platform-benchmarks-${{ matrix.os }}-${{ github.sha }}
        path: target/criterion/
        retention-days: 14

  benchmark-report:
    name: Generate Benchmark Report
    needs: [performance-benchmarks, memory-benchmarks, platform-benchmarks]
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download all benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        path: benchmark-artifacts

    - name: Install report dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3 python3-pip
        pip3 install matplotlib pandas numpy

    - name: Generate comprehensive report
      run: |
        # Create a comprehensive benchmark report
        python3 scripts/generate_report.py benchmark-artifacts/ > benchmark_report.md

    - name: Upload comprehensive report
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-benchmark-report-${{ github.sha }}
        path: |
          benchmark_report.md
          benchmark-artifacts/
        retention-days: 90

    - name: Create GitHub Release (for scheduled runs)
      if: github.event_name == 'schedule'
      uses: softprops/action-gh-release@v1
      with:
        tag_name: performance-baseline-${{ github.run_number }}
        name: Performance Baseline ${{ github.run_number }}
        body_path: benchmark_report.md
        files: |
          benchmark-artifacts/**/*
        draft: false
        prerelease: false