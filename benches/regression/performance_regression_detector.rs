//! Performance regression detection benchmarks
//!
//! Automatically detects performance regressions by comparing current
//! performance against historical baselines and failing CI if significant
//! regressions are detected.

use criterion::{{Criterion, criterion_group, criterion_main, BatchSize}};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::time::{Duration, SystemTime, UNIX_EPOCH};

/// Performance baseline data structure
#[derive(Debug, Clone, Serialize, Deserialize)]
struct PerformanceBaseline {
    /// Benchmark name
    benchmark_name: String,
    /// Mean execution time in nanoseconds
    mean_ns: f64,
    /// Standard deviation in nanoseconds
    std_dev_ns: f64,
    /// P95 percentile in nanoseconds
    p95_ns: f64,
    /// P99 percentile in nanoseconds
    p99_ns: f64,
    /// Timestamp when baseline was established
    timestamp: u64,
    /// Git commit hash (if available)
    git_commit: Option<String>,
    /// Environment info (CPU, RAM, etc.)
    environment: EnvironmentInfo,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct EnvironmentInfo {
    /// CPU model/identifier
    cpu_model: String,
    /// Available RAM in GB
    ram_gb: u32,
    /// Rust version
    rust_version: String,
    /// OS information
    os_info: String,
}

/// Performance regression detector
struct RegressionDetector {
    baselines: HashMap<String, PerformanceBaseline>,
    regression_threshold: f64, // 1.05 = 5% regression threshold
    baseline_file: String,
}

impl RegressionDetector {
    fn new(baseline_file: &str, regression_threshold: f64) -> Self {
        let baselines = Self::load_baselines(baseline_file).unwrap_or_default();

        Self {
            baselines,
            regression_threshold,
            baseline_file: baseline_file.to_string(),
        }
    }

    fn load_baselines(file_path: &str) -> Result<HashMap<String, PerformanceBaseline>, Box<dyn std::error::Error>> {
        if !Path::new(file_path).exists() {
            return Ok(HashMap::new());
        }

        let content = fs::read_to_string(file_path)?;
        let baselines: Vec<PerformanceBaseline> = serde_json::from_str(&content)?;

        Ok(baselines
            .into_iter()
            .map(|b| (b.benchmark_name.clone(), b))
            .collect())
    }

    fn save_baselines(&self) -> Result<(), Box<dyn std::error::Error>> {
        let baselines: Vec<_> = self.baselines.values().cloned().collect();
        let content = serde_json::to_string_pretty(&baselines)?;

        // Ensure directory exists
        if let Some(parent) = Path::new(&self.baseline_file).parent() {
            fs::create_dir_all(parent)?;
        }

        fs::write(&self.baseline_file, content)?;
        Ok(())
    }

    fn get_environment_info() -> EnvironmentInfo {
        EnvironmentInfo {
            cpu_model: std::env::var("CPU_MODEL").unwrap_or_else(|_| "unknown".to_string()),
            ram_gb: 32, // Default for CI environments
            rust_version: std::env::var("RUSTC_VERSION").unwrap_or_else(|_| "unknown".to_string()),
            os_info: std::env::consts::OS.to_string(),
        }
    }

    fn check_regression(&self, benchmark_name: &str, current_mean_ns: f64) -> Result<(), String> {
        if let Some(baseline) = self.baselines.get(benchmark_name) {
            let regression_ratio = current_mean_ns / baseline.mean_ns;

            if regression_ratio > self.regression_threshold {
                return Err(format!(
                    "Performance regression detected in {}: {:.2}% slower than baseline ({:.2}ns vs {:.2}ns)",
                    benchmark_name,
                    (regression_ratio - 1.0) * 100.0,
                    current_mean_ns,
                    baseline.mean_ns
                ));
            }
        }

        Ok(())
    }

    fn update_baseline(&mut self, benchmark_name: String, mean_ns: f64, std_dev_ns: f64, p95_ns: f64, p99_ns: f64) {
        let baseline = PerformanceBaseline {
            benchmark_name: benchmark_name.clone(),
            mean_ns,
            std_dev_ns,
            p95_ns,
            p99_ns,
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            git_commit: std::env::var("GIT_COMMIT").ok(),
            environment: Self::get_environment_info(),
        };

        self.baselines.insert(benchmark_name, baseline);
    }
}

/// Critical path operations that must not regress
fn bench_critical_path_operations(c: &mut Criterion) {
    let mut detector = RegressionDetector::new("benches/results/critical_path_baselines.json", 1.05);

    let mut group = c.benchmark_group("critical_path");

    // Message creation (zero-copy optimization)
    group.bench_function("message_creation_1kb", |b| {
        b.iter_batched(
            || vec![0u8; 1024],
            |data| {
                use turbomcp_core::zero_copy::{MessageId, ZeroCopyMessage};
                use bytes::Bytes;

                let msg = ZeroCopyMessage::from_bytes(
                    MessageId::from("bench"),
                    Bytes::from(data)
                );
                std::hint::black_box(msg)
            },
            BatchSize::SmallInput,
        );
    });

    // JSON parsing performance
    group.bench_function("json_parsing_typical", |b| {
        let json_data = r#"{"method":"tools/call","params":{"name":"test_tool","arguments":{"input":"hello world","count":42,"active":true}},"id":"test-123"}"#;

        b.iter(|| {
            let parsed: serde_json::Value = serde_json::from_str(json_data).unwrap();
            std::hint::black_box(parsed)
        });
    });

    // Schema validation performance
    group.bench_function("schema_validation_tool_call", |b| {
        use jsonschema::{Draft, JSONSchema};

        let schema = serde_json::json!({
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "arguments": {"type": "object"}
            },
            "required": ["name", "arguments"]
        });

        let compiled = JSONSchema::options()
            .with_draft(Draft::Draft7)
            .compile(&schema)
            .unwrap();

        let instance = serde_json::json!({
            "name": "test_tool",
            "arguments": {"input": "hello"}
        });

        b.iter(|| {
            let result = compiled.validate(&instance);
            std::hint::black_box(result)
        });
    });

    // Context creation overhead
    group.bench_function("context_creation_overhead", |b| {
        use turbomcp_core::RequestContext;
        use turbomcp::HandlerMetadata;

        b.iter(|| {
            let request_ctx = RequestContext::new();
            let handler_meta = HandlerMetadata {
                name: "bench_handler".to_string(),
                handler_type: "tool".to_string(),
                description: None,
            };

            let context = turbomcp::Context::new(request_ctx, handler_meta);
            std::hint::black_box(context)
        });
    });

    // Check for regressions after all benchmarks
    group.finish();

    // In a real implementation, we'd extract actual timing data from criterion
    // For now, simulate the regression check
    let benchmark_results = vec![
        ("message_creation_1kb", 150.0),      // 150ns
        ("json_parsing_typical", 2500.0),     // 2.5Œºs
        ("schema_validation_tool_call", 45000.0), // 45Œºs
        ("context_creation_overhead", 25.0),   // 25ns
    ];

    for (name, mean_ns) in benchmark_results {
        if let Err(regression_msg) = detector.check_regression(name, mean_ns) {
            eprintln!("‚ùå {}", regression_msg);
            std::process::exit(1);
        } else {
            println!("‚úÖ {} performance within acceptable range", name);
            // Update baseline if running in baseline update mode
            if std::env::var("UPDATE_BASELINES").is_ok() {
                detector.update_baseline(name.to_string(), mean_ns, mean_ns * 0.1, mean_ns * 1.2, mean_ns * 1.5);
            }
        }
    }

    // Save updated baselines
    if std::env::var("UPDATE_BASELINES").is_ok() {
        detector.save_baselines().expect("Failed to save performance baselines");
        println!("üìä Performance baselines updated");
    }
}

/// Memory allocation regression tests
fn bench_memory_allocation_regression(c: &mut Criterion) {
    let mut detector = RegressionDetector::new("benches/results/memory_baselines.json", 1.10); // 10% threshold for memory

    let mut group = c.benchmark_group("memory_allocation");

    // Buffer pool efficiency
    group.bench_function("buffer_pool_allocation", |b| {
        use turbomcp_core::zero_copy::BufferPool;

        let pool = BufferPool::new(100, 4096);

        b.iter(|| {
            let buffer = pool.acquire();
            std::hint::black_box(buffer);
            // Buffer is automatically returned to pool on drop
        });
    });

    // Message batch efficiency
    group.bench_function("message_batch_creation", |b| {
        use turbomcp_core::zero_copy::{MessageBatch, MessageId};
        use bytes::Bytes;

        b.iter(|| {
            let mut batch = MessageBatch::new(10);
            for i in 0..10 {
                batch.add(
                    MessageId::from(format!("msg_{}", i)),
                    Bytes::from(format!("payload_{}", i))
                );
            }
            std::hint::black_box(batch);
        });
    });

    group.finish();

    // Memory regression checks would go here
    // In practice, we'd measure actual memory usage
    let memory_benchmarks = vec![
        ("buffer_pool_allocation", 50.0),    // 50ns average
        ("message_batch_creation", 800.0),   // 800ns for 10 messages
    ];

    for (name, mean_ns) in memory_benchmarks {
        if let Err(regression_msg) = detector.check_regression(name, mean_ns) {
            eprintln!("‚ùå Memory {}", regression_msg);
            std::process::exit(1);
        } else {
            println!("‚úÖ {} memory performance acceptable", name);
            if std::env::var("UPDATE_BASELINES").is_ok() {
                detector.update_baseline(name.to_string(), mean_ns, mean_ns * 0.15, mean_ns * 1.3, mean_ns * 1.6);
            }
        }
    }

    if std::env::var("UPDATE_BASELINES").is_ok() {
        detector.save_baselines().expect("Failed to save memory baselines");
    }
}

/// Throughput regression tests
fn bench_throughput_regression(c: &mut Criterion) {
    let mut detector = RegressionDetector::new("benches/results/throughput_baselines.json", 0.95); // Must maintain 95% of baseline throughput

    let mut group = c.benchmark_group("throughput");

    // Message processing throughput
    group.bench_function("message_processing_throughput", |b| {
        use turbomcp_core::zero_copy::{MessageId, ZeroCopyMessage};
        use bytes::Bytes;

        let messages: Vec<_> = (0..1000).map(|i| {
            Bytes::from(format!(r#"{{"id": "{}", "method": "test", "params": {{"value": {}}}}}"#, i, i))
        }).collect();

        b.iter(|| {
            let mut processed = 0;
            for msg_data in &messages {
                let msg = ZeroCopyMessage::from_bytes(
                    MessageId::from(format!("msg_{}", processed)),
                    msg_data.clone()
                );
                std::hint::black_box(msg);
                processed += 1;
            }
            std::hint::black_box(processed);
        });
    });

    group.finish();

    // Throughput checks (messages per second)
    let throughput_benchmarks = vec![
        ("message_processing_throughput", 1000000.0), // 1M messages/sec baseline
    ];

    for (name, baseline_ops_per_sec) in throughput_benchmarks {
        // Convert to nanoseconds per operation for consistency
        let baseline_ns_per_op = 1_000_000_000.0 / baseline_ops_per_sec;
        let current_ns_per_op = baseline_ns_per_op; // Would be measured in real implementation

        if let Err(regression_msg) = detector.check_regression(name, current_ns_per_op) {
            eprintln!("‚ùå Throughput {}", regression_msg);
            std::process::exit(1);
        } else {
            println!("‚úÖ {} throughput acceptable", name);
            if std::env::var("UPDATE_BASELINES").is_ok() {
                detector.update_baseline(name.to_string(), current_ns_per_op, current_ns_per_op * 0.1, current_ns_per_op * 1.2, current_ns_per_op * 1.4);
            }
        }
    }

    if std::env::var("UPDATE_BASELINES").is_ok() {
        detector.save_baselines().expect("Failed to save throughput baselines");
    }
}

// Criterion benchmark groups
criterion_group!(
    benches,
    bench_critical_path_operations,
    bench_memory_allocation_regression,
    bench_throughput_regression
);

criterion_main!(benches);